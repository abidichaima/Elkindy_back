# -*- coding: utf-8 -*-
"""gpt4all.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G1dZIDm3HP4eWP9fmmnZRTdRgCwvx5TF
"""



from langchain_community.llms import GPT4All

model = GPT4All("C:/Users/user/Desktop/ggml-gpt4all-j-v1.3-groovy.bin")
output = model.generate("give me a python code ", max_tokens=250)
print(output)



from flask import Flask, request, jsonify
from langchain import PromptTemplate, LLMChain
from langchain.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

app = Flask(__name__)

template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])

local_path = 'C:/Users/user/Desktop/ggml-gpt4all-j-v1.3-groovy.bin'

callbacks = [StreamingStdOutCallbackHandler()]

llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)

llm_chain = LLMChain(prompt=prompt, llm=llm)

@app.route('/process_text', methods=['POST'])
def process_text():
    data = request.get_json()
    question = data['question']

    result = llm_chain.run(question)

    return jsonify({'result': result})

if __name__ == '__main__':
    app.run()

